{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La distribución previa como regularizador\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/1/18/Bayes%27_Theorem_MMB_01.jpg\" width=\"500px\" height=\"300px\" />\n",
    "\n",
    "> Un paso intermedio entre un enfoque 100% Bayesiano, en el que entendemos que los parámetros son la fuente de incertidumbre y por tanto debemos modelar su distribución de probabilidad, y el enfoque frecuentista, en el que suponemos parámetros fijos, es **modelar el cambio de nuestra intuición previa sobre los parámetros conforme observamos los datos**.\n",
    "  \n",
    "> Esto lo hacemos mediante la **aplicación de la regla de Bayes**.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Comprender el principio de estimación Máximum A-Posteriori (MAP).\n",
    "> - Entender el efecto de la distribución previa como regularizador.\n",
    "\n",
    "> **Referencias:**\n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera.\n",
    "> - Pattern Recognition and Machine Learning, by Christopher M. Bishop - Cap. 1.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Antes de comenzar, revisemos la distribución normal multivariable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Distribución normal multivariable\n",
    "\n",
    "A partir de la distribución normal que ya conocemos, podemos hacer una extensión natural a vectores, para obtener la función de densidad de probabilidad normal multivariada:\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu, \\Sigma) = \\frac{1}{\\sqrt{\\det(2 \\pi \\Sigma)}} \\exp \\left\\{-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right\\},\n",
    "$$\n",
    "\n",
    "con parámetros $\\mu \\in \\mathbb{R}^d$: vector de medias de la V.A. $X$ y $\\Sigma \\in \\mathbb{R}^{d \\times d}$: matriz de covarianzas de la VA X (simétrica y definida positiva)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo luce esta densidad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar scipy.stats\n",
    "from scipy import stats\n",
    "# Importar numpy\n",
    "import numpy as np\n",
    "# Importar matplitlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VA normales\n",
    "X = stats.multivariate_normal(mean=np.zeros(2),\n",
    "                              cov=np.eye(2))\n",
    "Y = stats.multivariate_normal(mean=np.array([2, 3]),\n",
    "                              cov=np.array([[1, 0.8],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y = np.linspace(-3, 3, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = X.pdf(np.dstack([x, y]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(x, y, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(x, y, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "x = np.linspace(-2, 6, 100)\n",
    "y = np.linspace(-3, 9, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "z = Y.pdf(np.dstack([x, y]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(x, y, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(x, y, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ajuste de curvas desde una perspectiva probabilística - revisitado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partimos de nuevo modelando la relación entre nuestra variable de salida $y$ y las variables de entrada $x$ como un **modelo lineal** con incertidumbre, la cual suponemos normal:\n",
    "\n",
    "$$\n",
    "y = \\phi(x)^T w + \\epsilon,\n",
    "$$\n",
    "\n",
    "con $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalentemente, podemos decir que:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(\\phi(x)^T w, \\beta^{-1}),\n",
    "$$\n",
    "\n",
    "con lo que\n",
    "\n",
    "$$\n",
    "p(y | x, w) = \\mathcal{N}(y | \\phi(x)^T w, \\beta^{-1}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya sabemos a donde nos conduce este modelo para la función de verosimilitud. \n",
    "\n",
    "Alternativamente, encontremos una expresión para la distribución posterior de los parámetros, usando la regla de Bayes:\n",
    "\n",
    "\\begin{align}\n",
    "p(w | y, X) & = \\frac{p(y, w | X)}{p(y | X)} \\qquad \\text{(Definición de probabilidad condicional)} \\\\\n",
    "            & = \\frac{p(y | X, w) p(w | X)}{p(y | X)} \\qquad \\text{(Bayes)}.\n",
    "\\end{align}\n",
    "\n",
    "En este contexto, tenemos que:\n",
    "\n",
    "- $p(w | y, X)$ es la distribución posterior de los parámetros dados los datos.\n",
    "\n",
    "- $p(y | X, w) = \\mathcal{L}(w) = \\prod_{i=1}^{N} \\mathcal{N}(y_i | \\phi(x_i)^T w, \\beta^{-1})$ es la función de verosimilitud.\n",
    "\n",
    "- $p(y | X)$ es la distribución de evidencia.\n",
    "\n",
    "- $p(w | X)$ es la distribución previa. Para este punto, una suposición que hace bastante sentido es que conocer únicamente las variables de entrada $X$ no nos dice absolutamente nada acerca de los parámetros $w$. Es decir, **son independientes**. Por tanto,\n",
    "\n",
    "  $$\n",
    "  p(w | X) = p(w).\n",
    "  $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de parámetros: MAP\n",
    "\n",
    "Entonces, un enfoque alternativo al principio de máxima verosimilitud es estimar los parámetros con su valor **más probable**, una vez conocemos los datos, e involucrando el conocimiento previo que tengamos de los parámetros.\n",
    "\n",
    "Esto es, maximizamos la distribución posterior:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{w}_{MAP} & = \\arg \\max_{w} p(w | y, X) \\\\\n",
    "              & = \\arg \\max_{w} \\frac{p(y | X, w) p(w | X)}{p(y | X)} \\\\\n",
    "              & = \\arg \\max_{w} p(y | X, w) p(w | X) \\qquad \\text{(La distribución de evidencia no depende de $w$)} \\\\\n",
    "              & = \\arg \\max_{w} \\log p(y | X, w) + \\log p(w) \\qquad \\text{(El logaritmo es una función creciente y cóncava)} \\\\\n",
    "              & = \\arg \\max_{w} l(w) + \\log p(w)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El término de log-verosimilitud ya lo conocemos:\n",
    "\n",
    "$$\n",
    "l(w) = \\frac{N}{2}\\log\\beta - \\frac{N}{2}\\log(2 \\pi) - \\frac{\\beta}{2} \\left|\\left|y - \\Phi w\\right|\\right|^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pregunta es, **¿Qué es la distribución previa $p(w)$?**\n",
    "\n",
    "- Este término nos permite expresar todo nuestro conocimiento previo acerca de los parámetros $w$ de una manera probabilística, es decir, a través de una distribución de probabilidad.\n",
    "\n",
    "#### Previa normal\n",
    "\n",
    "Una elección común es:\n",
    "  \n",
    "\\begin{align}\n",
    "p(w) & = \\mathcal{N}(w | 0, \\alpha^{-1} I) \\\\\n",
    "   & = \\frac{1}{\\sqrt{\\det(2 \\pi \\alpha^{-1} I)}} \\exp \\left\\{-\\frac{\\alpha}{2} w^T w\\right\\} \\\\\n",
    "   & = \\frac{\\alpha^{d / 2}}{(2 \\pi)^{d / 2}} \\exp \\left\\{-\\frac{\\alpha}{2} \\left|\\left|w\\right|\\right|^2\\right\\}. \\\\\n",
    "\\end{align}\n",
    "\n",
    "En este caso, podemos usar el parámetro $\\alpha$ para expresar cuanta certeza tenemos de que $w$ está cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo anterior, observamos que:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(w) & = \\log \\frac{\\alpha^{d / 2}}{(2 \\pi)^{d / 2}} -  \\frac{\\alpha}{2} \\left|\\left|w\\right|\\right|^2\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reemplazando en la expresión anterior y descartando todos los sumandos que no dependen de $w$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{w}_{MAP} & = \\arg \\max_{w} l(w) + \\log p(w) \\\\\n",
    "              & = \\arg \\max_{w} - \\frac{\\beta}{2} \\left|\\left|y - \\Phi w\\right|\\right|^2 - \\frac{\\alpha}{2} \\left|\\left|w\\right|\\right|^2 \\\\\n",
    "              & = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2 + \\underbrace{\\frac{\\alpha}{\\beta}}_{\\lambda} \\left|\\left|w\\right|\\right|^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, observamos que la estimación de parámetros por MAP, usando una previa Gaussiana nos conduce a nuestra intuición detrás de **mínimos cuadrados regularizados con norma-$2$ (Ridge)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previa de Laplace\n",
    "\n",
    "Otra previa común en este caso, es modelar la incertidumbre en los parámetros como una distribución de Laplace:\n",
    "\n",
    "$$\n",
    "p(w_i) = \\frac{1}{2 b} \\exp\\left(-\\frac{|w_i|}{b}\\right),\n",
    "$$\n",
    "\n",
    "con lo cual, suponiendo que los parámetros son independientes:\n",
    "\n",
    "\\begin{align}\n",
    "p(w) & = \\prod_{i=1}^{d} p(w_i) \\\\\n",
    "     & = \\frac{1}{(2 b)^{d}} \\exp\\left(-\\sum_{i=1}^{d}\\frac{|w_i|}{b}\\right) \\\\\n",
    "     & = \\frac{1}{(2 b)^{d}} \\exp\\left(-\\frac{1}{b} \\sum_{i=1}^{d}|w_i|\\right). \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, observamos que:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(w) & = \\log \\frac{1}{(2 b)^{d}} - \\frac{1}{b} \\sum_{i=1}^{d}|w_i|,\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con lo cual\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{w}_{MAP} & = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2 + \\underbrace{\\frac{1}{\\beta b}}_{\\lambda}  \\sum_{i=1}^{d}|w_i|.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Entonces, observamos que la estimación de parámetros por MAP, usando una previa de Laplace nos conduce a nuestra intuición detrás de **mínimos cuadrados regularizados con norma-$1$ (Lasso)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusiones\n",
    "\n",
    "1. Usando una perspectiva probabilística, recuperamos resultados intuitivos básicos en ajuste de curvas.\n",
    "\n",
    "2. Vimos que los estimadores de máxima verosimilitud son propensos a overfitting si tenemos pocos datos.\n",
    "\n",
    "3. También, vimos que los estimadores tipo MAP, al introducir la distribución previa (en la que expresamos nuestro conocimiento a priori de los parámetros), tiene un efecto de regularización.\n",
    "\n",
    "4. Tenemos herramientas poderosas para estimar parámetros de modelos probabilísticos, sin embargo, solo estamos haciendo estimaciones puntuales de los parámetros. Una pregunta es: ¿Qué tan seguro estás de tu estimación?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
