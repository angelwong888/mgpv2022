{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimadores de máxima verosimilitud\n",
    "\n",
    "![MLE](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/MLfunctionbinomial-en.svg/600px-MLfunctionbinomial-en.svg.png)\n",
    "\n",
    "> La estimación por máxima verosimilitud (maximum likelihood estimation - MLE) es uno de los métodos más utilizados para estimar los parámetros de un modelo probabilístico.\n",
    "\n",
    "> La idea básica, es elegir los parámetros que maximizan la función de verosimilitud. Intuitivamente, esto corresponde a elegir los parámetros que maximizan la probabilidad de los datos observados.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Comprender el principio de máxima verosimilitud a través de ejemplos básicos.\n",
    "> - Estimar los parámetros de algunas distribuciones comunes usando el principio de máxima verosimilitud.\n",
    "> - Entender las limitaciones básicas de los estimadores de máxima verosimilitud.\n",
    "\n",
    "Referencia:\n",
    "\n",
    "- http://www.cs.toronto.edu/~rgrosse/csc321/probabilistic_models.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principios básicos\n",
    "\n",
    "**Retomamos el ejemplo de la moneda**.\n",
    "\n",
    "Definimos la variable aleatoria (discreta) binomial:\n",
    "\n",
    "$$\n",
    "X = \\left\\{ \\begin{array}{ccc} 1 & \\text{si} & \\text{la moneda cae cara} \\\\\n",
    "                               0 & \\text{si} & \\text{la moneda cae sello} \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "con función de masa de probabilidad dada por:\n",
    "\n",
    "- $P(X=1) = P(X=1 | \\theta) = \\theta$;\n",
    "- $P(X=0) = P(X=0 | \\theta) = 1 - P(X=1) = 1 - \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, suponemos que $X_1, X_2, \\dots, X_N$ representan las variables aleatorias correspondientes a $N$ tiros de la moneda. Estas variables aleatorias son **independientes**, e **idénticamente distribuidas** (i.i.d.).\n",
    "\n",
    "> En muchos contextos (incluidas las aplicaciones de aprendizaje de máquinas) se supone que las diferentes observaciones de los datos son i.i.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ¿cuál es la probabilidad de observar determinada sucesión de resultados de tiros de la moneda suponiendo que esta sigue la distribución de probabilidad dada arriba?\n",
    "\n",
    "Supongamos que observamos $\\{x_1, x_2, \\dots, x_N\\}$, donde $x_i = 0, 1$ para todo $i=1, \\dots, N$. Queremos entonces averiguar la probabilidad:\n",
    "\n",
    "$$\n",
    "P(X_1=x_1, X_2=x_2, \\dots, X_N=x_N | \\theta)\n",
    "$$\n",
    "\n",
    "> Esta probabilidad es una función del parámetro $\\theta$ (desconocido) y corresponde a la **verosimilitud de la muestra** $\\{x_1, x_2, \\dots, x_N\\}$, la cual denotamos por:\n",
    "  \n",
    "  $$\n",
    "  \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) = P(X_1=x_1, X_2=x_2, \\dots, X_N=x_N | \\theta).\n",
    "  $$\n",
    "  \n",
    "> Intuitivamente, la función de verosimilitud la podemos interpretar como la probabilidad de los datos dados los parámetros. El **principio de máxima verosimilitud** establece que un estimador para los parámetros es aquel que **maximiza la probabilidad de los datos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como suponemos que las variables son **independientes**, tenemos que:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) & = P(X_1=x_1, X_2=x_2, \\dots, X_N=x_N | \\theta) \\\\\n",
    "                                          & = P(X_1=x_1 | \\theta) P(X_2=x_2| \\theta) \\dots P(X_N=x_N | \\theta) \\\\\n",
    "                                          & = \\prod_{j=1}^{N} P(X_j = x_j | \\theta).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, analicemos el término $P(X_j = x_j | \\theta)$. ¿Qué posibles valores puede tomar?\n",
    "\n",
    "$$\n",
    "P(X_j = x_j | \\theta) = \\left\\{ \\begin{array}{ccc} \\theta   & \\text{si} & x_j=1 \\\\\n",
    "                                                   1-\\theta & \\text{si} & x_j=0 \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De modo que la función de verosimilitud solo depende de cuantas veces de los $N$ tiros cayó cara, y cuantas veces de los $N$ tiros cayó sello.\n",
    "\n",
    "Denotemos por:\n",
    "\n",
    "- $N_0$: Número de veces que cayó sello.\n",
    "\n",
    "- $N_1$: Número de veces que cayó cara.\n",
    "\n",
    "Se debe satisfacer, claramente, que $N_0 + N_1 = N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) = \\theta^{N_1} (1 - \\theta)^{N_0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**\n",
    "\n",
    "Supongamos que tiramos la moneda 10 veces, y obtenemos:\n",
    "\n",
    "$$\n",
    "\\{0, 0, 1, 0, 1, 1, 1, 1, 0, 1\\}\n",
    "$$\n",
    "\n",
    "En este caso:\n",
    "- $N_0 = 4$\n",
    "- $N_1 = 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una idea razonable es encontrar $\\theta$ de manera que **la función verosimilitud sea máxima**, o equivalentemente, tal que **se maximice la probabilidad de la muestra observada**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos numpy\n",
    "\n",
    "# Importamos matplotlib.pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la función de verosimilitud con los datos del ejemplo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujamos la función de verosimilitud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aproximamos el valor máximo de la verosimilitud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál es el valor óptimo para $\\theta$?\n",
    "\n",
    "- Como dicta la intuición es ... 6 / 10 = 0.6\n",
    "- Como dicta la intuición es ... 60 / 100 = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, lo que nos dice el principio de máxima verosimilitud es: elegir $\\theta$ tal que\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\arg \\max_{\\theta} \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) = \\theta^{N_1} (1 - \\theta)^{N_0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truco común:\n",
    "\n",
    "- El logaritmo (natural) es una función creciente y cóncava.\n",
    "  - Por tanto,\n",
    "    \n",
    "    $$\n",
    "    \\arg \\max_{\\theta} \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) = \\arg \\max_{\\theta} \\underbrace{\\log \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N)}_{l(\\theta; x_1, x_2, \\dots, x_N): \\text{ log-verosimilitud}}\n",
    "    $$\n",
    "    \n",
    "- La **log-verosimilitud** ofrece la ventaja de convertir las multiplicaciones en sumas:\n",
    "  \n",
    "  \\begin{align}\n",
    "  l(\\theta; x_1, x_2, \\dots, x_N) & = \\log \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) \\\\\n",
    "                                  & = \\log \\left(\\theta^{N_1} (1 - \\theta)^{N_0} \\right) \\\\\n",
    "                                  & = N_1 \\log \\theta + N_0 \\log(1 - \\theta).\n",
    "  \\end{align}\n",
    "  \n",
    "  y en general, las sumas son más fáciles de derivar (y maximizar) que los productos.\n",
    "  \n",
    "> En general, la función de verosimilitud para muestras i.i.d. es:\n",
    "> \n",
    "> $$\n",
    "  \\mathcal{L}(\\theta; x_1, x_2, \\dots, x_N) = \\prod_{j=1}^{N} P(X_j = x_j | \\theta)\n",
    "  $$\n",
    ">  \n",
    "> de manera que la log-verosimilitud será:\n",
    ">\n",
    "> $$\n",
    "  \\mathcal{l}(\\theta; x_1, x_2, \\dots, x_N) = \\sum_{j=1}^{N} \\log P(X_j = x_j | \\theta)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green> Maximizar la log-verosimilitud para el ejemplo de la moneda en el pizarrón. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parámetros de máxima verosimilitud en una distribución normal (tarea)\n",
    "\n",
    "En este caso tenemos:\n",
    "\n",
    "- Variable aleatoria $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "- $p(x | \\mu,\\sigma^2)=\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponemos que las observaciones\n",
    "\n",
    "$$\\mathcal{D} = \\{x_1, \\dots, x_N\\}$$\n",
    "\n",
    "son independientes e idénticamente distribuidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar que los estimadores de máxima verosimilitud de $\\mu$ y $\\sigma$ son:\n",
    "\n",
    "$$\\hat{\\mu}_{MLE} = \\frac{1}{N} \\sum_{j=1}^{N}x_j \\qquad \\text{y} \\qquad \\hat{\\sigma}_{MLE} = \\sqrt{\\frac{1}{N}\\sum_{j=1}^{N}(x_j-\\hat{\\mu}_{MLE})^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parámetros de máxima verosimilitud en una distribución de Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En este caso tenemos:\n",
    "\n",
    "- Variable aleatoria $X \\sim L(\\mu, b)$.\n",
    "\n",
    "- $p(x | \\mu,b)=\\frac{1}{2 b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponemos que las observaciones\n",
    "\n",
    "$$\\mathcal{D} = \\{x_1, \\dots, x_N\\}$$\n",
    "\n",
    "son independientes e idénticamente distribuidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar que los estimadores de máxima verosimilitud de $\\mu$ y $\\sigma$ son:\n",
    "\n",
    "$$\\hat{\\mu}_{MLE} = \\text{mediana}(\\mathcal{D}) \\qquad \\text{y} \\qquad \\hat{b}_{MLE} = \\frac{1}{N}\\sum_{j=1}^{N}|x_j-\\hat{\\mu}_{MLE}|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comentarios finales\n",
    "\n",
    "El principio de máxima verosimilitud es bastante poderoso, y además una técnica general para estimar los parámetros de un modelo probabilístico. \n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Sin embargo, tiene un problema: **en caso de tener pocos datos de entrenamiento, podemos sobreajustar seriamente el modelo.**\n",
    "\n",
    "### Suposición básica\n",
    "\n",
    "El principio de máxima verosimilitud es bastante intuitivo: estimar los parámetros de manera que se maximice la probabilidad de los datos. Esto trae consigo la suposición subyacente de que los parámetros **son fijos**, de manera que la incertidumbre proviene de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
